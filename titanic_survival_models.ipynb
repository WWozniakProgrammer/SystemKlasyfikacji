{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db13fb87",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddee97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess the Titanic dataset\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "\n",
    "# Select features and target\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "le_sex = LabelEncoder()\n",
    "data['Sex'] = le_sex.fit_transform(data['Sex'])\n",
    "\n",
    "le_embarked = LabelEncoder()\n",
    "data['Embarked'] = le_embarked.fit_transform(data['Embarked'])\n",
    "\n",
    "# Prepare dataset\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train‑test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66766e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Soft k‑NN ---\n",
    "class SoftKNN:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = np.array(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = []\n",
    "            for i, x_train in enumerate(self.X_train):\n",
    "                dist = np.linalg.norm(x - x_train)\n",
    "                distances.append((dist, self.y_train[i]))\n",
    "            distances.sort(key=lambda tup: tup[0])\n",
    "            neighbors = distances[:self.k]\n",
    "            weights = [1 / (d + 1e-5) for d, _ in neighbors]\n",
    "            votes = {}\n",
    "            for (_, label), weight in zip(neighbors, weights):\n",
    "                votes[label] = votes.get(label, 0) + weight\n",
    "            pred = max(votes.items(), key=lambda x: x[1])[0]\n",
    "            predictions.append(pred)\n",
    "        return predictions\n",
    "\n",
    "# --- Naive Bayes (manual) ---\n",
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.means = {}\n",
    "        self.vars = {}\n",
    "        self.priors = {}\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.means[c] = np.mean(X_c, axis=0)\n",
    "            self.vars[c] = np.var(X_c, axis=0) + 1e-6\n",
    "            self.priors[c] = X_c.shape[0] / X.shape[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "            for c in self.classes:\n",
    "                prior = np.log(self.priors[c])\n",
    "                likelihood = -0.5 * np.sum(np.log(2 * np.pi * self.vars[c]))\n",
    "                likelihood -= 0.5 * np.sum(((x - self.means[c]) ** 2) / self.vars[c])\n",
    "                posteriors.append(prior + likelihood)\n",
    "            preds.append(self.classes[np.argmax(posteriors)])\n",
    "        return preds\n",
    "\n",
    "# --- Decision Tree (simple recursive) ---\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1]\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        best_split = None\n",
    "        parent_entropy = self._entropy(y)\n",
    "        for feat in range(self.n_features):\n",
    "            thresholds = np.unique(X[:, feat])\n",
    "            for t in thresholds:\n",
    "                left = y[X[:, feat] <= t]\n",
    "                right = y[X[:, feat] > t]\n",
    "                if len(left) == 0 or len(right) == 0:\n",
    "                    continue\n",
    "                gain = parent_entropy - (len(left) / len(y)) * self._entropy(left) - (len(right) / len(y)) * self._entropy(right)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_split = (feat, t)\n",
    "        return best_split\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        if len(set(y)) == 1 or depth == self.max_depth:\n",
    "            return int(np.bincount(y).argmax())\n",
    "        split = self._best_split(X, y)\n",
    "        if not split:\n",
    "            return int(np.bincount(y).argmax())\n",
    "        feat, t = split\n",
    "        left_idx = X[:, feat] <= t\n",
    "        right_idx = X[:, feat] > t\n",
    "        left_tree = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
    "        right_tree = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
    "        return (feat, t, left_tree, right_tree)\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        if isinstance(node, int):\n",
    "            return node\n",
    "        feat, t, left, right = node\n",
    "        if x[feat] <= t:\n",
    "            return self._predict_one(x, left)\n",
    "        else:\n",
    "            return self._predict_one(x, right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_one(x, self.tree) for x in X]\n",
    "\n",
    "# --- Fuzzy Set Voting k‑NN ---\n",
    "def fuzzy_vote_knn(X_train, y_train, X_test, k):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    predictions = []\n",
    "    for x in X_test:\n",
    "        dists = np.linalg.norm(X_train - x, axis=1)\n",
    "        indices = np.argsort(dists)[:k]\n",
    "        votes = {}\n",
    "        for i in indices:\n",
    "            label = y_train[i]\n",
    "            mu = 1 / (1 + dists[i])\n",
    "            votes[label] = votes.get(label, 0) + mu\n",
    "        pred = max(votes.items(), key=lambda x: x[1])[0]\n",
    "        predictions.append(pred)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Evaluate different k values ---\n",
    "k_values = range(1, 16, 2)\n",
    "soft_knn_results = []\n",
    "fuzzy_knn_results = []\n",
    "\n",
    "for k in k_values:\n",
    "    soft_knn = SoftKNN(k=k)\n",
    "    soft_knn.fit(X_train, y_train)\n",
    "    y_pred_soft = soft_knn.predict(X_test)\n",
    "    acc_soft = accuracy_score(y_test, y_pred_soft)\n",
    "    soft_knn_results.append((k, acc_soft))\n",
    "\n",
    "    y_pred_fuzzy = fuzzy_vote_knn(X_train, y_train, X_test, k=k)\n",
    "    acc_fuzzy = accuracy_score(y_test, y_pred_fuzzy)\n",
    "    fuzzy_knn_results.append((k, acc_fuzzy))\n",
    "\n",
    "best_soft_k, best_soft_acc = max(soft_knn_results, key=lambda x: x[1])\n",
    "best_fuzzy_k, best_fuzzy_acc = max(fuzzy_knn_results, key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nBest Soft k‑NN Accuracy: {best_soft_acc:.4f} with k = {best_soft_k}\")\n",
    "print(f\"Best Fuzzy k‑NN Accuracy: {best_fuzzy_acc:.4f} with k = {best_fuzzy_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Train models with optimal k and evaluate ---\n",
    "soft_knn = SoftKNN(k=best_soft_k)\n",
    "soft_knn.fit(X_train, y_train)\n",
    "y_pred_soft = soft_knn.predict(X_test)\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "tree = DecisionTree(max_depth=4)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "y_pred_fuzzy = fuzzy_vote_knn(X_train, y_train, X_test, k=best_fuzzy_k)\n",
    "\n",
    "models = {\n",
    "    \"Soft k‑NN\": y_pred_soft,\n",
    "    \"Naive Bayes\": y_pred_nb,\n",
    "    \"Decision Tree\": y_pred_tree,\n",
    "    \"Fuzzy k‑NN\": y_pred_fuzzy\n",
    "}\n",
    "\n",
    "for name, preds in models.items():\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Plot accuracy vs k ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot([k for k, _ in soft_knn_results], [acc for _, acc in soft_knn_results], label='Soft k‑NN')\n",
    "plt.plot([k for k, _ in fuzzy_knn_results], [acc for _, acc in fuzzy_knn_results], label='Fuzzy k‑NN')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs k for Soft k‑NN and Fuzzy k‑NN')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
